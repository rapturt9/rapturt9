# Hi, I'm Ram Potham 👋

I'm an **AI Safety Researcher** at the MIT Algorithmic Alignment Lab

* 🎓 **Google Scholar:** [Research](https://scholar.google.com/citations?user=Uc-rKk0AAAAJ&hl=en)
* 🌐 **Portfolio:** [rampotham.com](https://rampotham.com)
* 📄 **LinkedIn:** [/in/rampotham](https://linkedin.com/in/rampotham)
* 🐦 **X:** [@PothamRam](https://twitter.com/PothamRam)

---

### About Me

I'm focused on ensuring the development of advanced AI leads to a safe and prosperous future. My perspective is shaped by my prior experience as the founder of a VC-backed startup where I built autonomous AI agents. This gave me a firsthand understanding of the rapid progress and potential risks in AI, motivating me to pivot my career to focus on them. My research focuses on mitigating existential risk from AI.

---

### 🚀 Featured Publications

* **Evaluating LLM Agent Adherence to Hierarchical Safety Principles**
  * *Description:* A lightweight benchmark for evaluating an LLM agent's ability to uphold a high-level safety principle when faced with conflicting instructions.
  * *Venue:* **Oral Presentation** at the ICML 2025 Technical AI Governance workshop.
  * ➡️ **[Read the paper on arXiv (2506.02357)](https://arxiv.org/abs/2506.02357)**

* **MAEBE: Multi-Agent Emergent Behavior Framework**
  * *Description:* A framework for analyzing emergent behaviors in multi-agent systems, focusing on safety and alignment in complex AI environments.
  * *Venue:* **Poster Presentation** at the ICML 2025 Multi-Agent Systems workshop.
  * ➡️ **[Read the paper on arXiv (2506.03053)](https://arxiv.org/abs/2506.03053)**

---

### 💻 Tech Stack & Skills

* **AI Safety Concepts:** Empirical Alignment, Safety Evaluations, Robustness Testing, Guardrails, Safety Cases
* **Languages & Frameworks:** Python, PyTorch, TensorFlow
* **ML & Engineering:** Multi-Agent Systems, LLM Fine-Tuning, AWS, Cloud Architecture
