# Ram Potham

ðŸ‘‹ Hi, I'm Ram! I'm a recent graduate from Carnegie Mellon University with a B.S. in Artificial Intelligence (Dec 2024).

My core focus is on **AI Safety research**, specifically working towards **Guaranteed Safe AI** through robust **AI Control** and advancing **LLM Corrigibility**. I believe that ensuring advanced AI systems are controllable, correctable, and aligned with human values is one of the most critical challenges of our time, and I'm dedicated to contributing technical solutions to this space. My motivation stems from a desire to see AI developed as a profoundly beneficial technology for humanity, helping to solve major global challenges and foster greater compassion.

---

### ðŸš€ What I'm Focused On:

* **AI Corrigibility:** Developing methodologies to build and verify AI systems (especially LLMs) that reliably accept human correction, defer to guidance, and allow for safe modification. I'm currently exploring this through my [Research Agenda: Advancing LLM Corrigibility]([link_to_your_agenda_if_public_or_remove_link](https://docs.google.com/document/d/e/2PACX-1vRxTi37-RoPZIRUTfzH6MxHLKztQq7b6mOCrj2-twHHdZTc6tHJ1l1t4prx9MB0IVfWN0XejjgsPcz2/pub)) and in collaboration with Jobst Heitzig on formalizing "power" as a proxy for corrigibility in Gridworld environments.
* **AI Control:** Designing and evaluating mechanisms to ensure that even highly capable or potentially misaligned AI systems remain under robust human control and cannot cause catastrophic harm. This includes research into high-assurance monitoring and intervention systems.
* **Guaranteed Safe AI:** Investigating techniques, including those inspired by formal methods and verification, to provide stronger assurances and guarantees about the safety and behavior of AI systems.

### ðŸŒ± My Background & Experience:

* **Education:** B.S. in Artificial Intelligence, Carnegie Mellon University (School of Computer Science).
* **Entrepreneurship:** Founder/CEO/CTO at **Sitewiz**, where I developed autonomous AI agents for analytics and UI/UX automation, focusing on ensuring their alignment with business KPIs and safe deployment. This involved practical work in LLM verification and control.
* **Research:**
    * AI/HCI Researcher at CMU's Chimps Lab, contributing to **WeAudit**, an open science crowd-auditing framework for identifying AI robustness failures and biases.
    * AI Researcher on enhancing fashion e-commerce with AI, involving VLM fine-tuning (Multi-LoRA on Stable Diffusion) and failure mode investigation.
* **AI Safety Engagement:**
    * Teaching Assistant for the Alignment Research Fellowship (AI Safety Global Society), teaching the ARENA curriculum.
    * AI Safety Fundamentals Transformative AI Certification from Bluedot.
* **Key Skills:** AI Safety & Interpretability (Robustness Testing, Guardrails, Preference Alignment), ML (Python, PyTorch, TensorFlow), Agentic Systems, Cloud (AWS).

### ðŸ”— Connect with Me:

* **LinkedIn:** [linkedin.com/in/rampotham](https://linkedin.com/in/rampotham)
* **AI Control Literature Review:** [AI Control LessWrong Post](https://www.lesswrong.com/posts/3PBvKHB2EmCujet3j/ai-control-methods-literature-review)]

### ðŸ’¡ Currently Exploring:

* Techniques for formalizing and using corrigibility
* Methods for creating verifiable components within larger AI systems.
* The intersection of interpretability (like Representation Engineering) and AI control.

---

Thanks for stopping by! I'm always open to discussing ideas and potential collaborations in AI safety.
