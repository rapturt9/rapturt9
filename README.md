# Hi, I'm Ram Potham 👋

I'm an **AI Safety Researcher & CBAI Fellow at MIT/Harvard** focused on the technical alignment of advanced AI systems.

* 🌐 **Portfolio:** [rampotham.com](https://rampotham.com)
* 📄 **LinkedIn:** [/in/rampotham](https://linkedin.com/in/rampotham)
* 🐦 **X:** [@PothamRam](https://twitter.com/PothamRam)
* 🎓 **Google Scholar:** [Research](https://scholar.google.com/citations?user=Uc-rKk0AAAAJ&hl=en)

---

### About Me

I'm focused on ensuring the development of advanced AI leads to a safe and prosperous future. My perspective is shaped by my prior experience as the founder of a VC-backed startup where I built autonomous AI agents. This gave me a firsthand understanding of the rapid progress and potential risks in AI, motivating me to pivot my career to focus on them. This work is grounded in my B.S. in Artificial Intelligence from Carnegie Mellon and my current research as a CBAI Fellow.

---

### 🔭 My Core Research: ForecastLabs

My primary research direction is **ForecastLabs**, an initiative to provide the strategic foresight needed to effectively reduce AI risk. The mission is to help steer humanity toward a safer world by answering critical questions about the future of AI.

* 🗺️ **Mapping the Future of AI:** Building a quantitative, predictive map of the AI landscape to identify the most effective interventions for reducing risk.
* 🧭 **Improving Strategic Foresight:** Producing actionable analysis for the researchers, funders, and policymakers making critical decisions about AI.
* 📈 **Developing Advanced Forecasting Systems:** Creating and aggregating predictive models to rival human superforecasters on questions vital to the safe development of AI.

---

### 🚀 Featured Publications

While my current focus is on strategic foresight, my background includes technical work on agent alignment:

* **Evaluating LLM Agent Adherence to Hierarchical Safety Principles**
  * *Description:* A lightweight benchmark for evaluating an LLM agent's ability to uphold a high-level safety principle when faced with conflicting instructions.
  * *Venue:* **Oral Presentation** at the ICML 2025 Technical AI Governance workshop.
  * ➡️ **[Read the paper on arXiv (2506.02357)](https://arxiv.org/abs/2506.02357)**

* **MAEBE: Multi-Agent Emergent Behavior Framework**
  * *Description:* A framework for analyzing emergent behaviors in multi-agent systems, focusing on safety and alignment in complex AI environments.
  * *Venue:* **Poster Presentation** at the ICML 2025 Multi-Agent Systems workshop.
  * ➡️ **[Read the paper on arXiv (2506.03053)](https://arxiv.org/abs/2506.03053)**

---

### 💻 Tech Stack & Skills

* **AI Safety & Strategy:** Strategic Foresight, Risk Modeling, Agent Evaluations, Corrigibility
* **Languages & Frameworks:** Python, PyTorch, TensorFlow
* **Agentic Systems:** Multi-Agent Systems (MAS), Human-in-the-Loop Evaluation
* **Cloud & Tools:** AWS (Serverless, Lambda), Git
