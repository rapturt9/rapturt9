# Hi, I'm Ram Potham 👋

I'm an **AI Safety Researcher & CBAI Fellow at MIT/Harvard** focused on the technical alignment of advanced AI systems.

* 🌐 **Portfolio:** [rampotham.com](https://rampotham.com)
* 📄 **LinkedIn:** [/in/rampotham](https://linkedin.com/in/rampotham)
* 🐦 **X:** [@PothamRam](https://twitter.com/PothamRam)
* 🎓 **Google Scholar:** [Research](https://scholar.google.com/citations?user=Uc-rKk0AAAAJ&hl=en)

---

### About Me

I'm focused on solving the core technical challenges of AI alignment to prevent loss-of-control scenarios. My perspective is shaped by my prior experience as the founder of a VC-backed startup where I built autonomous AI agents, which gave me a firsthand understanding of the urgent need for robust safety protocols. This work is grounded in my B.S. in Artificial Intelligence from Carnegie Mellon and my current research as a CBAI Fellow.

---

### 🚀 Featured Publications

Here are some of my recent research contributions:

* **Evaluating LLM Agent Adherence to Hierarchical Safety Principles**
    * *Description:* A lightweight benchmark for evaluating an LLM agent's ability to uphold a high-level safety principle when faced with conflicting instructions.
    * *Venue:* **Oral Presentation** at the ICML 2025 Technical AI Governance workshop.
    * ➡️ **[Read the paper on arXiv (2506.02357)](https://arxiv.org/abs/2506.02357)**

* **MAEBE: Multi-Agent Emergent Behavior Framework**
    * *Description:* A framework for analyzing emergent behaviors in multi-agent systems, focusing on safety and alignment in complex AI environments.
    * *Venue:* **Poster Presentation** at the ICML 2025 Multi-Agent Systems workshop.
    * ➡️ **[Read the paper on arXiv (2506.03053)](https://arxiv.org/abs/2506.03053)**

---

### 💡 My Research Focus

My primary research interests are centered around ensuring AI systems are robustly controllable and correctable:

* 🤖 **AI Corrigibility:** Developing methods to build and verify AI systems (especially LLMs) that reliably accept human correction and allow for safe modification.
* ⚙️ **AI Control:** Designing and evaluating mechanisms to ensure even highly capable AI systems remain under robust human control and cannot cause catastrophic harm.
* 🔒 **Guaranteed Safe AI:** Investigating techniques inspired by formal methods to provide stronger, verifiable assurances about the safety and behavior of AI systems.

---

### 💻 Tech Stack & Skills

* **AI Safety:** Corrigibility, Agent Evaluations, Robustness Testing, Guardrails, Preference Alignment
* **Languages & Frameworks:** Python, PyTorch, TensorFlow
* **Agentic Systems:** Multi-Agent Systems (MAS), Human-in-the-Loop Evaluation
* **Cloud & Tools:** AWS (Serverless, Lambda), Git

---

### 🌱 Accountability

I publish my personal daily standups publicly to maintain accountability and share my process. Feel free to look or copy the template:
* ➡️ **[My Daily Standups](https://docs.google.com/document/d/e/2PACX-1vSgDIsCVPauhlCFDTuU0TITb7Lj44d1b9DdtvxcgNJDi3YAXN_QJvmH9wRhvsMP9gOakoJXH4Ye3DSd/pub)**
