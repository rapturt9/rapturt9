# Hi, I'm Ram Potham 👋

I'm an **AI Safety Researcher** at the MIT Algorithmic Alignment Lab

* 🌐 **Portfolio:** [rampotham.com](https://rampotham.com)
* 📄 **LinkedIn:** [/in/rampotham](https://linkedin.com/in/rampotham)
* 🐦 **X:** [@PothamRam](https://twitter.com/PothamRam)
* 🎓 **Google Scholar:** [Research](https://scholar.google.com/citations?user=Uc-rKk0AAAAJ&hl=en)

---

### About Me

I'm focused on ensuring the development of advanced AI leads to a safe and prosperous future. My perspective is shaped by my prior experience as the founder of a VC-backed startup where I built autonomous AI agents. This gave me a firsthand understanding of the rapid progress and potential risks in AI, motivating me to pivot my career to focus on them. My research focuses on mitigating existential risk from AI.

---

### 🚀 Featured Publications

While my current focus is on strategic foresight, my background includes technical work on agent alignment:

* **Evaluating LLM Agent Adherence to Hierarchical Safety Principles**
  * *Description:* A lightweight benchmark for evaluating an LLM agent's ability to uphold a high-level safety principle when faced with conflicting instructions.
  * *Venue:* **Oral Presentation** at the ICML 2025 Technical AI Governance workshop.
  * ➡️ **[Read the paper on arXiv (2506.02357)](https://arxiv.org/abs/2506.02357)**

* **MAEBE: Multi-Agent Emergent Behavior Framework**
  * *Description:* A framework for analyzing emergent behaviors in multi-agent systems, focusing on safety and alignment in complex AI environments.
  * *Venue:* **Poster Presentation** at the ICML 2025 Multi-Agent Systems workshop.
  * ➡️ **[Read the paper on arXiv (2506.03053)](https://arxiv.org/abs/2506.03053)**

---

### 💻 Tech Stack & Skills

* **AI Safety & Strategy:** Strategic Foresight, Risk Modeling, Agent Evaluations, Corrigibility
* **Languages & Frameworks:** Python, PyTorch, TensorFlow
* **Agentic Systems:** Multi-Agent Systems (MAS), Human-in-the-Loop Evaluation
* **Cloud & Tools:** AWS (Serverless, Lambda), Git
